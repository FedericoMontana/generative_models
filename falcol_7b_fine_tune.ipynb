{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/workspace/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /workspace/generative_models/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.9\n",
      "CUDA SETUP: Detected CUDA version 117\n",
      "CUDA SETUP: Loading binary /workspace/generative_models/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda117.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/generative_models/.venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "  warn(msg)\n",
      "/workspace/generative_models/.venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/workspace/generative_models/.venv/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "/workspace/generative_models/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '41GB'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "max_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:59<00:00, 29.53s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    load_in_8bit=True,\n",
    "    device_map={\"\":0},\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tiiuae/falcon-7b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4718592 || all params: 6926439296 || trainable%: 0.06812435363037071\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 295768960 0.042701444040779475\n",
      "torch.int8 6625951744 0.9566173124229168\n",
      "torch.float32 4718592 0.0006812435363037072\n"
     ]
    }
   ],
   "source": [
    "# Verifying the datatypes.\n",
    "dtypes = {}\n",
    "for _, p in model.named_parameters():\n",
    "    dtype = p.dtype\n",
    "    if dtype not in dtypes:\n",
    "        dtypes[dtype] = 0\n",
    "    dtypes[dtype] += p.numel()\n",
    "total = 0\n",
    "for k, v in dtypes.items():\n",
    "    total += v\n",
    "for k, v in dtypes.items():\n",
    "    print(k, v, v / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract, load, and transform OpenAssistant/oasst1 chatbot dataset\n",
    "\n",
    "# set some pandas options to make the output more readable\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.width\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/workspace/hf/datasets/OpenAssistant___parquet/OpenAssistant--oasst1-2960c57d7e52ab15/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00,  6.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# load dataset from huggingface datasets\n",
    "ds = load_dataset(\"OpenAssistant/oasst1\")\n",
    "# lets convert the train dataset to a pandas df\n",
    "df = ds[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets grab the message trees to train on \n",
    "message_tree_ids = np.unique(np.array(df[\"message_tree_id\"]))\n",
    "messages = {}\n",
    "messages['message_tree_id'] = []\n",
    "messages['message_tree_text'] = []\n",
    "\n",
    "for message_tree_id in message_tree_ids:\n",
    "    try:\n",
    "        # look at all data for this message tree\n",
    "        one_message_tree = df.query(f\"message_tree_id == '{message_tree_id}'\").sort_values(\"created_date\")\n",
    "        text = \"\"\n",
    "    \n",
    "        # root message\n",
    "        text += \"<human>: \" + one_message_tree.iloc[0].text\n",
    "        # find root message's children\n",
    "        children = one_message_tree[one_message_tree.parent_id == one_message_tree.iloc[0].message_id]\n",
    "        # find root message's top ranked child:\n",
    "        child = children[children['rank'] == 0.0]\n",
    "        text += '\\n' + \"<bot>: \" + child.iloc[0].text\n",
    "    \n",
    "        # proceed through rest of the above message tree until completion\n",
    "        flag=True\n",
    "        while flag:\n",
    "            try:\n",
    "                # find next prompt\n",
    "                children = one_message_tree[one_message_tree.parent_id == child.message_id.iloc[0]]\n",
    "                children.index\n",
    "                one_message_tree.loc[children.index].iloc[0].role\n",
    "                text += '\\n' + \"<human>: \" + one_message_tree.loc[children.index].iloc[0].text\n",
    "    \n",
    "                # find next children\n",
    "                children = one_message_tree[one_message_tree.parent_id == one_message_tree.loc[children.index].iloc[0].message_id]\n",
    "                children\n",
    "                # find top ranked child:\n",
    "                child = children[children['rank'] == 0.0]\n",
    "                text += '\\n' + \"<bot>: \" + child.iloc[0].text\n",
    "            except:\n",
    "                flag=False\n",
    "    \n",
    "        messages['message_tree_id'].append(message_tree_id)\n",
    "        messages['message_tree_text'].append(text)\n",
    "\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "message_df = pd.DataFrame.from_dict(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<human>: What is the capital of Japan?\n",
      "<bot>: Tokyo is the capital of Japan.\n",
      "<human>: And what is the capital of the Moon?\n",
      "<bot>: Since the Moon is not currently inhabited by humans, there is currently no capital of the Moon.\n",
      "\n",
      "\n",
      "message_tree_id                   01505c0f-2d68-4206-acfc-7ab39033c75a\n",
      "message_tree_text    <human>: What is the capital of Japan?\\n<bot>:...\n",
      "Name: 41, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# check some random messages\n",
    "\n",
    "i=41\n",
    "print(message_df.message_tree_text.iloc[i])\n",
    "print('\\n')\n",
    "print(message_df.iloc[i])\n",
    "\n",
    "# check whole dataset\n",
    "\n",
    "# message_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
